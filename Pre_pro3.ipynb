{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitin/anaconda3/envs/san/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from random import shuffle, seed\n",
    "import sys\n",
    "import os.path\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "import scipy.io\n",
    "import pdb\n",
    "import string\n",
    "import h5py\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(sentence):\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) if i!='' and i!=' ' and i!='\\n'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'My', 'name', 'is', 'Nitin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'Hi, My name is Nitin'\n",
    "tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'My', 'name', 'is', 'Nitin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(string) #NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_question(imgs, params):\n",
    "    print ('example processed tokens:')\n",
    "    for i,img in enumerate(imgs):\n",
    "        #print(i)\n",
    "        #print(img'\\n')\n",
    "        s = img['question']\n",
    "        #print(s)\n",
    "        if params['token_method'] == 'nltk':\n",
    "            txt = word_tokenize(str(s).lower())\n",
    "            #print(txt'\\n')\n",
    "        else:\n",
    "            txt = tokenize(s)\n",
    "            #print(txt'\\n')\n",
    "        img['processed_tokens'] = txt\n",
    "        if i < 10: print (txt)\n",
    "        if i % 1000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done)   \\r\" %  (i, len(imgs), i*100.0/len(imgs)) )\n",
    "            sys.stdout.flush()   \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short snippets corresponding to `prepro_question()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/vqa_raw_train.json') as js:\n",
    "    imgs_train = json.load(js)\n",
    "    #imgs_train = json.load(open('input_train_json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    s = img['question']\n",
    "    #print(i, s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "What is this photo taken looking through?\n",
    "What position is this man playing?\n",
    "What color is the players shirt?\n",
    "Is this man a professional baseball player?\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    s = img['question']\n",
    "    txt = word_tokenize(str(s).lower())\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "['what', 'is', 'this', 'photo', 'taken', 'looking', 'through', '?']\n",
    "['what', 'position', 'is', 'this', 'man', 'playing', '?']\n",
    "['what', 'color', 'is', 'the', 'players', 'shirt', '?']\n",
    "['is', 'this', 'man', 'a', 'professional', 'baseball', 'player', '?']\n",
    "['what', 'color', 'is', 'the', 'snow', '?']\n",
    "['what', 'is', 'the', 'person', 'doing', '?']\n",
    "['what', 'color', 'is', 'the', 'persons', 'headwear', '?']\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    print(\"processing %d/%d (%.2f%% done)   \\r\" %  (i, len(imgs_train), i*100.0/len(imgs_train)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "processing 0/443757 (0.00% done)   \n",
    "processing 1/443757 (0.00% done)   \n",
    "processing 2/443757 (0.00% done)   \n",
    "processing 3/443757 (0.00% done)   \n",
    "processing 4/443757 (0.00% done)   \n",
    "processing 5/443757 (0.00% done)   \n",
    "processing 6/443757 (0.00% done)   \n",
    "processing 7/443757 (0.00% done)   \n",
    "processing 8/443757 (0.00% done)   \n",
    "processing 9/443757 (0.00% done)   \n",
    "processing 10/443757 (0.00% done)\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_question(imgs, params):\n",
    "    # build vocabulary for question and answers.\n",
    "\n",
    "    count_thr = params['word_count_threshold']\n",
    "\n",
    "    # count up the number of words\n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "        for w in img['processed_tokens']:\n",
    "            counts[w] = counts.get(w, 0) + 1\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "    print ('top words and their counts:')\n",
    "    print ('\\n'.join(map(str,cw[:20])))\n",
    "\n",
    "    # print some stats\n",
    "    total_words = sum(counts.values())\n",
    "    print ('total words:', total_words)\n",
    "    bad_words = [w for w,n in counts.items() if n <= count_thr]\n",
    "    vocab = [w for w,n in counts.items() if n > count_thr]\n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    print ('number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "    print ('number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print ('number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))\n",
    "\n",
    "\n",
    "    # lets now produce the final annotation\n",
    "    # additional special UNK token we will use below to map infrequent words to\n",
    "    print ('inserting the special UNK token')\n",
    "    vocab.append('UNK')\n",
    "  \n",
    "    for img in imgs:\n",
    "        txt = img['processed_tokens']\n",
    "        question = [w if counts.get(w,0) > count_thr else 'UNK' for w in txt]\n",
    "        img['final_question'] = question\n",
    "\n",
    "    return imgs, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `build_vocab_question()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of words in the tokens\n",
    "\n",
    "counts = {}\n",
    "#for img in imgs:   \n",
    "#    for w in txt:\n",
    "#        counts[w] = counts.get(w, 0) + 1\n",
    "        \n",
    "for i, img in enumerate(imgs_train):\n",
    "    s = img['question']\n",
    "    txt = word_tokenize(str(s).lower())\n",
    "    \n",
    "    img['processed_tokens'] = txt\n",
    "    for w in txt:\n",
    "        counts[w] = counts.get(w, 0) + 1\n",
    "       # print('%s : %s' % (w, counts[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "many : 14450\n",
    "roads : 22\n",
    "does : 6645\n",
    "this : 25464\n",
    "have : 3471\n",
    "? : 130628\n",
    "is : 81764\n",
    "the : 95072\n",
    "hillside : 12\n",
    "steep : 10\n",
    "? : 130629\n",
    "do : 2983\n",
    "they : 2276\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words and their counts:\n",
      "(443931, '?')\n",
      "(322836, 'the')\n",
      "(277570, 'is')\n",
      "(185348, 'what')\n",
      "(104094, 'are')\n"
     ]
    }
   ],
   "source": [
    "# sort the words in order to get the top words\n",
    "\n",
    "cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "print ('Top words and their counts:')\n",
    "print ('\\n'.join(map(str, cw[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 3216813\n"
     ]
    }
   ],
   "source": [
    "total_words = sum(counts.values())\n",
    "print ('total words:', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_thr = 0 # > 0 to see changes\n",
    "bad_words = [w for w,n in counts.items() if n <= count_thr]\n",
    "vocab = [w for w,n in counts.items() if n > count_thr]\n",
    "bad_count = sum(counts[w] for w in bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bad words: 0/13771 = 0.00%\n",
      "number of words in vocab would be 13771\n",
      "number of UNKs: 0/3216813 = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print ('number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "print ('number of words in vocab would be %d' % (len(vocab), ))\n",
    "print ('number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "itow = {i+1:w for i,w in enumerate(vocab)} # a 1-indexed vocab translation table\n",
    "wtoi = {w:i+1 for i,w in enumerate(vocab)} # inverse table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(wtoi, open('word_to_ix_test.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNK : used to replace the rare words that did not fit in the vocab. Eg: Sentence `My name is guotong1988` will be translated into `My name is _unk_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('inserting the special UNK token')\n",
    "vocab.append('UNK')\n",
    "\n",
    "for img in imgs_train:\n",
    "    txt = word_tokenize(str(s).lower())\n",
    "    img['processed_tokens'] = txt\n",
    "    \n",
    "    question = [w if counts.get(w,0) > count_thr else 'UNK' for w in img['processed_tokens']]\n",
    "    img['final_question'] = question\n",
    "    \n",
    "    #for w in img['processed_tokens']:\n",
    "    #    if counts.get(w, 0) > count_thr:\n",
    "    #        question = w\n",
    "    #    else:\n",
    "    #        question = 'UNK'\n",
    "            \n",
    "    #img['final_question'] = question\n",
    "    \n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "inserting the special UNK token\n",
    "\n",
    "{'ques_id': 458752000, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What is this photo taken looking through?', 'ans': 'net', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "\n",
    "{'ques_id': 458752001, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What position is this man playing?', 'ans': 'pitcher', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "\n",
    "{'ques_id': 458752002, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What color is the players shirt?', 'ans': 'orange', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "\n",
    "{'ques_id': 458752003, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'Is this man a professional baseball player?', 'ans': 'yes', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "\n",
    "{'ques_id': 262146000, 'img_path': 'train2014/COCO_train2014_000000262146.jpg', 'question': 'What color is the snow?', 'ans': 'white', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "\n",
    "{'ques_id': 262146001, 'img_path': 'train2014/COCO_train2014_000000262146.jpg', 'question': 'What is the person doing?', 'ans': 'skiing', 'processed_tokens': ['is', 'that', 'a', 'laptop', '?'], 'final_question': ['is', 'that', 'a', 'laptop', '?']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_answers(imgs, params):\n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "        ans = img['ans'] \n",
    "        counts[ans] = counts.get(ans, 0) + 1\n",
    "\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "    print ('top answer and their counts:')    \n",
    "    print ('\\n'.join(map(str,cw[:20])))\n",
    "    \n",
    "    vocab = []\n",
    "    for i in range(params['num_ans']):\n",
    "        vocab.append(cw[i][1])\n",
    "\n",
    "    return vocab[:params['num_ans']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `get_top_answers()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for img in imgs_train:\n",
    "    ans = img['ans'] \n",
    "    counts[ans] = counts.get(ans, 0) + 1\n",
    "    print(\"%s : %s\" %(ans, counts[ans]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "net : 1\n",
    "pitcher : 1\n",
    "orange : 1\n",
    "yes : 1\n",
    "white : 1\n",
    "skiing : 1\n",
    "red : 1\n",
    "frisbee : 1\n",
    "yes : 2\n",
    "frisbee : 2\n",
    "yes : 3\n",
    "yes : 4\n",
    "contrail : 1\n",
    "yes : 5\n",
    "white and purple : 1\n",
    "brushing teeth : 1\n",
    "yes : 6\n",
    "no : 1\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top answer and their counts:\n",
      "(443931, '?')\n",
      "(322836, 'the')\n",
      "(277570, 'is')\n",
      "(185348, 'what')\n",
      "(104094, 'are')\n"
     ]
    }
   ],
   "source": [
    "cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "print ('top answer and their counts:')    \n",
    "print ('\\n'.join(map(str,cw[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ans = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "the\n",
      "is\n",
      "what\n",
      "are\n",
      "this\n",
      "in\n",
      "on\n",
      "of\n",
      "a\n",
      "how\n",
      "many\n",
      "color\n",
      "there\n",
      "man\n",
      "does\n",
      "people\n",
      "picture\n",
      "'s\n",
      "to\n",
      "wearing\n",
      "it\n",
      "these\n",
      "where\n",
      "person\n",
      "have\n",
      "kind\n",
      "or\n",
      "photo\n",
      "do\n",
      "you\n",
      "woman\n",
      "doing\n",
      "type\n",
      "animal\n",
      "they\n",
      "room\n",
      "be\n",
      "holding\n",
      "animals\n",
      "can\n",
      "for\n",
      "dog\n",
      "cat\n",
      "at\n",
      "train\n",
      "his\n",
      "he\n",
      "sign\n",
      "that\n",
      "which\n",
      "any\n",
      "food\n",
      "shirt\n",
      "bus\n",
      "water\n",
      "see\n",
      "an\n",
      "why\n",
      "made\n",
      "playing\n",
      "plane\n",
      "sitting\n",
      "shown\n",
      "plate\n",
      "table\n",
      "right\n",
      "with\n",
      "time\n",
      "left\n",
      "boy\n",
      "taken\n",
      "sport\n",
      "background\n",
      "was\n",
      "number\n",
      "standing\n",
      "pizza\n",
      "girl\n",
      "white\n",
      "look\n",
      "who\n",
      "from\n",
      "being\n",
      "has\n",
      "her\n",
      "wall\n",
      "street\n",
      "all\n",
      "and\n",
      "sky\n",
      "she\n",
      "hand\n",
      "day\n",
      "ground\n",
      "top\n",
      "men\n",
      "say\n",
      "visible\n",
      "looking\n",
      "red\n",
      "clock\n",
      "eating\n",
      "building\n",
      "would\n",
      "truck\n",
      "image\n",
      "child\n",
      "like\n",
      "here\n",
      "front\n",
      "bear\n",
      "up\n",
      "toilet\n",
      "name\n",
      "going\n",
      "out\n",
      "one\n",
      "behind\n",
      "bird\n",
      "game\n",
      "same\n",
      "car\n",
      "fruit\n",
      "green\n",
      "riding\n",
      "horse\n",
      "seen\n",
      "light\n",
      "trees\n",
      "black\n",
      "object\n",
      "scene\n",
      "giraffe\n",
      "their\n",
      "bed\n",
      "hair\n",
      "hat\n",
      "colors\n",
      "brand\n",
      "elephant\n",
      "two\n",
      "vehicle\n",
      "cake\n",
      "pictured\n",
      "ball\n",
      "side\n",
      "floor\n",
      "flowers\n",
      "blue\n",
      "could\n",
      "glasses\n",
      "bike\n",
      "did\n",
      "head\n",
      "been\n",
      "lights\n",
      "used\n",
      "grass\n",
      "bathroom\n",
      "boat\n",
      "window\n",
      "baby\n",
      "someone\n",
      "open\n",
      "giraffes\n",
      "sunny\n",
      "player\n",
      "horses\n",
      "old\n",
      "cars\n",
      "next\n",
      "computer\n",
      "bench\n",
      "down\n",
      "shoes\n",
      "elephants\n",
      "meal\n",
      "birds\n",
      "umbrella\n",
      ",\n",
      "back\n",
      "zebras\n",
      "road\n",
      "outside\n",
      "yellow\n",
      "both\n",
      "happy\n",
      "women\n",
      "guy\n",
      "fence\n",
      "zebra\n",
      "flying\n",
      "camera\n",
      "than\n",
      "appear\n",
      "phone\n",
      "lady\n",
      "snow\n",
      "kitchen\n",
      "under\n",
      "weather\n",
      "tree\n",
      "laptop\n",
      "hydrant\n",
      "windows\n",
      "tennis\n",
      "shape\n",
      "about\n",
      "will\n",
      "pants\n",
      "anyone\n",
      "bowl\n",
      "air\n",
      "by\n",
      "eat\n",
      "around\n",
      "traffic\n",
      "motorcycle\n",
      "written\n",
      "called\n",
      "clouds\n",
      "place\n",
      "clean\n",
      "vase\n",
      "sandwich\n",
      "hanging\n",
      "glass\n",
      "tie\n",
      "door\n",
      "sheep\n",
      "direction\n",
      "pattern\n",
      "more\n",
      "real\n",
      "cows\n",
      "city\n",
      "moving\n",
      "fire\n",
      "other\n",
      "likely\n",
      "hands\n",
      "different\n",
      "area\n",
      "house\n",
      "inside\n",
      "team\n",
      "hot\n",
      "tv\n",
      "kite\n",
      "beach\n",
      "chair\n",
      "face\n",
      "feet\n",
      "country\n",
      "laying\n",
      "over\n",
      "using\n",
      "walking\n",
      "dogs\n",
      "children\n",
      "facing\n",
      "bananas\n",
      "signs\n",
      "boats\n",
      "above\n",
      "off\n",
      "mirror\n",
      "bears\n",
      "material\n",
      "stop\n",
      "jacket\n",
      "item\n",
      "sink\n",
      "eyes\n",
      "helmet\n",
      "items\n",
      "batter\n",
      "dish\n",
      "near\n",
      "frisbee\n",
      "cold\n",
      "orange\n",
      "chairs\n",
      "cow\n",
      "most\n",
      "skateboard\n",
      "taking\n",
      "played\n",
      "players\n",
      "long\n",
      "vegetable\n",
      "umbrellas\n",
      "season\n",
      "planes\n",
      "little\n",
      "those\n",
      "bottom\n",
      "parked\n",
      "board\n",
      "night\n",
      "full\n",
      "large\n",
      "girls\n",
      "way\n",
      "raining\n",
      "healthy\n",
      "middle\n",
      "boys\n",
      "shorts\n",
      "kid\n",
      "vehicles\n",
      "restaurant\n",
      "desk\n",
      "wild\n",
      "new\n",
      "make\n",
      "big\n",
      "vegetables\n",
      "think\n",
      "park\n",
      "meat\n",
      "walls\n",
      "clear\n",
      "kids\n",
      "good\n",
      "cats\n",
      "paper\n",
      "drinking\n",
      "wheels\n",
      "cup\n",
      "stove\n",
      "sun\n",
      "daytime\n",
      "as\n",
      "home\n",
      "company\n",
      "field\n",
      "need\n",
      "not\n",
      "sleeping\n",
      "cloudy\n",
      "fruits\n",
      "surfboard\n",
      "use\n",
      "couch\n",
      "skiing\n",
      "each\n",
      "play\n",
      "bikes\n",
      "high\n",
      "zoo\n",
      "airplane\n",
      "get\n",
      "running\n",
      "carrying\n",
      "bottle\n",
      "part\n",
      "word\n",
      "screen\n",
      "buses\n",
      "objects\n",
      "lot\n",
      "if\n",
      "event\n",
      "seat\n",
      "small\n",
      "baseball\n",
      "match\n",
      "plates\n",
      "tall\n",
      "counter\n",
      "something\n",
      "closed\n",
      "flower\n",
      "into\n",
      "hit\n",
      "foreground\n",
      "box\n",
      "thing\n",
      "smiling\n",
      "kites\n",
      "tracks\n",
      "types\n",
      "surfer\n",
      "present\n",
      "modern\n",
      "shower\n",
      "refrigerator\n",
      "trying\n",
      "dressed\n",
      "stripes\n",
      "young\n",
      "just\n",
      "wine\n",
      "motion\n",
      "leaves\n",
      "professional\n",
      "having\n",
      "its\n",
      "bag\n",
      "tower\n",
      "numbers\n",
      "drink\n",
      "teddy\n",
      "ready\n",
      "activity\n",
      "donuts\n",
      "bat\n",
      "watching\n",
      "flag\n",
      "store\n",
      "neck\n",
      "empty\n",
      "waiting\n",
      "much\n",
      "main\n",
      "fridge\n",
      "female\n",
      "located\n",
      "transportation\n",
      "parking\n",
      "oven\n",
      "photograph\n",
      "painted\n",
      "buildings\n",
      "style\n",
      "keyboard\n",
      "court\n",
      "covering\n",
      "brown\n",
      "mouth\n",
      "letter\n",
      "slices\n",
      "cut\n",
      "things\n",
      "letters\n",
      "male\n",
      "pillows\n",
      "wet\n",
      "ski\n",
      "natural\n",
      "mouse\n",
      "shadow\n",
      "suitcase\n",
      "corner\n",
      "closest\n",
      "design\n",
      "touching\n",
      "far\n",
      "skier\n",
      "go\n",
      "plant\n",
      "should\n",
      "pictures\n",
      "year\n",
      "legs\n",
      "coffee\n",
      "tail\n",
      "hats\n",
      "breed\n",
      "eaten\n",
      "first\n",
      "dress\n",
      "coming\n",
      "trains\n",
      "through\n",
      "toy\n",
      "getting\n",
      "pink\n",
      "adult\n",
      "television\n",
      "device\n",
      "sidewalk\n",
      "motorcycles\n",
      "take\n",
      "shirts\n",
      "bread\n",
      "racket\n",
      "doors\n",
      "between\n",
      "stuffed\n",
      "turned\n",
      "anything\n",
      "wear\n",
      "body\n",
      "fall\n",
      "towels\n",
      "book\n",
      "making\n",
      "gender\n",
      "cutting\n",
      "pieces\n",
      "size\n",
      "dark\n",
      "language\n",
      "microwave\n",
      "poles\n",
      "when\n",
      "waves\n",
      "utensil\n",
      "reflection\n",
      "wood\n",
      "surface\n",
      "station\n",
      "might\n",
      "cooked\n",
      "mountains\n",
      "lamp\n",
      "surfing\n",
      "airline\n",
      "recently\n",
      "indoors\n",
      "overcast\n",
      "center\n",
      "banana\n",
      "safe\n",
      "pole\n",
      "pointing\n",
      "coat\n",
      "show\n",
      "uniform\n",
      "lines\n",
      "luggage\n",
      "line\n",
      "knife\n",
      "cheese\n",
      "ceiling\n",
      "sunglasses\n",
      "scissors\n",
      "lit\n",
      "``\n",
      "skateboarder\n",
      "position\n",
      "depicted\n",
      "candles\n",
      "work\n",
      "fork\n",
      "logo\n",
      "hotel\n",
      "everyone\n",
      "ocean\n",
      "plants\n",
      "appliance\n",
      "belong\n",
      "bottles\n",
      "tell\n",
      "some\n",
      "driving\n",
      "computers\n",
      "socks\n",
      "shot\n",
      "bridge\n",
      "donut\n",
      "cabinets\n",
      "''\n",
      "n't\n",
      "landing\n",
      "fast\n",
      "suit\n",
      "curtains\n",
      "photographer\n",
      "tiles\n",
      "them\n",
      "away\n",
      "vegetarian\n",
      "passenger\n",
      "displayed\n",
      "curtain\n",
      "attached\n",
      "so\n",
      "collar\n",
      "served\n",
      "apples\n",
      "dirty\n",
      "sand\n",
      "metal\n",
      "enough\n",
      "arm\n",
      "race\n",
      "license\n",
      "writing\n",
      "setting\n",
      "trash\n",
      "gear\n",
      "skiers\n",
      "jumping\n",
      "working\n",
      "stuff\n",
      "lid\n",
      "trucks\n",
      "towel\n",
      "system\n",
      "sort\n",
      "showing\n",
      "clocks\n",
      "books\n",
      "colored\n",
      "words\n",
      "graffiti\n",
      "cooking\n",
      "purpose\n",
      "lying\n",
      "covered\n",
      "cell\n",
      "live\n",
      "wave\n",
      "still\n",
      "birthday\n",
      "mountain\n",
      "warm\n",
      "advertised\n",
      "clothing\n",
      "only\n",
      "utensils\n",
      "outfit\n",
      "three\n",
      "mean\n",
      "jersey\n",
      "too\n",
      "read\n",
      "meter\n",
      "call\n",
      "alone\n",
      "piece\n",
      "outdoors\n",
      "yet\n",
      "skis\n",
      "deep\n",
      "container\n",
      "winter\n",
      "ride\n",
      "breakfast\n",
      "land\n",
      "trick\n",
      "calm\n",
      "beverage\n",
      "well\n",
      "living\n",
      "clothes\n",
      "leaning\n",
      "close\n",
      "vases\n",
      "tablecloth\n",
      "roof\n",
      "ripe\n",
      "ears\n",
      "display\n",
      "business\n",
      "remote\n",
      "cups\n",
      "sit\n",
      "normal\n",
      "busy\n",
      "pulling\n",
      "handle\n",
      "couple\n",
      "benches\n",
      "location\n",
      "furniture\n",
      "carrots\n",
      "blanket\n",
      "asleep\n",
      "structure\n",
      "rain\n",
      "shelf\n",
      "summer\n",
      "'\n",
      "cast\n",
      "short\n",
      "fun\n",
      "round\n",
      "pitcher\n",
      "were\n",
      "toppings\n",
      "probably\n",
      "missing\n",
      "catch\n",
      "broccoli\n",
      "monitor\n",
      "flags\n",
      "human\n",
      "laptops\n",
      "featured\n",
      "very\n",
      "apple\n",
      "airport\n",
      "basket\n",
      "rider\n",
      "public\n",
      "family\n",
      "traveling\n",
      "throwing\n",
      "blurry\n",
      "resting\n",
      "dessert\n",
      "party\n",
      "foot\n",
      "wrist\n",
      "taller\n",
      "purple\n",
      "lamps\n",
      "hill\n",
      "habitat\n",
      "pic\n",
      "arrow\n",
      "distance\n",
      "talking\n",
      "gloves\n",
      "bicycle\n",
      "office\n",
      "stand\n",
      "painting\n",
      "fireplace\n",
      "colorful\n",
      "dinner\n",
      "shining\n",
      "seem\n",
      "cabinet\n",
      "american\n",
      "cross\n",
      "straight\n",
      "we\n",
      "snowing\n",
      "adults\n",
      "flat\n",
      "driver\n",
      "america\n",
      "species\n",
      "cap\n",
      "represent\n",
      "heads\n",
      "jet\n",
      "during\n",
      "watch\n",
      "skating\n",
      "plastic\n",
      "guys\n",
      "gray\n",
      "carpet\n",
      "suitcases\n",
      "pizzas\n",
      "pillow\n",
      "holiday\n",
      "against\n",
      "walk\n",
      "swimming\n",
      "liquid\n",
      "catcher\n",
      "tile\n",
      "engine\n",
      "sad\n",
      "beer\n",
      "rug\n",
      "fly\n",
      "seats\n",
      "tusks\n",
      "english\n",
      "skateboarding\n",
      "kinds\n",
      "foods\n",
      "wetsuit\n",
      "united\n",
      "skater\n",
      "salad\n",
      "oranges\n",
      "hungry\n",
      "beside\n",
      "shadows\n",
      "dry\n",
      "last\n",
      "pot\n",
      "focus\n",
      "states\n",
      "machine\n",
      "equipment\n",
      "tray\n",
      "falling\n",
      "whose\n",
      "sinks\n",
      "bright\n",
      "wooden\n",
      "cones\n",
      "sauce\n",
      "dishes\n",
      "alive\n",
      "safety\n",
      "considered\n",
      "below\n",
      "turn\n",
      "rocks\n",
      "police\n",
      "helmets\n",
      "handed\n",
      "surfboards\n",
      "spoon\n",
      "grazing\n",
      "your\n",
      "nose\n",
      "school\n",
      "now\n",
      "half\n",
      "bowls\n",
      "backpack\n",
      "video\n",
      "flooring\n",
      "view\n",
      "towards\n",
      "together\n",
      "pan\n",
      "age\n",
      "throw\n",
      "symbol\n",
      "post\n",
      "model\n",
      "beds\n",
      "environment\n",
      "crowded\n",
      "phones\n",
      "across\n",
      "van\n",
      "represented\n",
      "desert\n",
      "silver\n",
      "group\n",
      "crossing\n",
      "come\n",
      "ear\n",
      "character\n",
      "teeth\n",
      "set\n",
      "river\n",
      "passengers\n",
      "nearby\n",
      "celebrating\n",
      "soda\n",
      "snowboard\n",
      "gas\n",
      "appliances\n",
      "tires\n",
      "screens\n",
      "monitors\n",
      "know\n",
      "free\n",
      "bathtub\n",
      "track\n",
      "sofa\n",
      "commercial\n",
      "bags\n",
      "messy\n",
      "lunch\n",
      "jeans\n",
      "electronic\n",
      "doughnuts\n",
      "arms\n",
      "shade\n",
      "mug\n",
      "humans\n",
      "edible\n",
      "dominant\n",
      "tag\n",
      "horns\n",
      "contain\n",
      "state\n",
      "mode\n",
      "frame\n",
      "currently\n",
      "seated\n",
      "market\n",
      "ladies\n",
      "want\n",
      "statue\n",
      "fit\n",
      "military\n",
      "faces\n",
      "end\n",
      "stripe\n",
      "2\n",
      "art\n",
      "older\n",
      "double\n",
      "common\n",
      "snowboarding\n",
      "shop\n",
      "mirrors\n",
      "lake\n",
      "fur\n",
      "dangerous\n",
      "sale\n",
      "participating\n",
      "nighttime\n",
      "grown\n",
      "flavor\n",
      "sliced\n",
      "job\n",
      "dirt\n",
      "church\n",
      "besides\n",
      "us\n",
      "surfers\n",
      "skirt\n",
      "nice\n",
      "makes\n",
      "leg\n",
      "already\n",
      "vest\n",
      "tub\n",
      "shelves\n",
      "placed\n",
      "growing\n",
      "boots\n",
      "aircraft\n",
      "toothbrush\n",
      "reflected\n",
      "paint\n",
      "married\n",
      "intersection\n",
      "shoe\n",
      "toys\n",
      "stands\n",
      "spot\n",
      "able\n",
      "striped\n",
      "related\n",
      "facial\n",
      "second\n",
      "farm\n",
      "engines\n",
      "whole\n",
      "put\n",
      "forest\n",
      "another\n",
      "trunk\n",
      "topping\n",
      "sticking\n",
      "hold\n",
      "comfortable\n",
      "town\n",
      "electric\n",
      "devices\n",
      "blinds\n",
      "before\n",
      "space\n",
      "sandwiches\n",
      "posing\n",
      "jump\n",
      "cover\n",
      "skate\n",
      "urban\n",
      "goggles\n",
      "catching\n",
      "restroom\n",
      "spots\n",
      "sold\n",
      "owner\n",
      "cart\n",
      "bicycles\n",
      "reading\n",
      "power\n",
      "fresh\n",
      "toilets\n",
      "platform\n",
      "signal\n",
      "done\n",
      "sweater\n",
      "rolls\n",
      "him\n",
      "ethnicity\n",
      "downhill\n",
      "allowed\n",
      "chocolate\n",
      "rural\n",
      "recent\n",
      "beard\n",
      "tomatoes\n",
      "steps\n",
      "movie\n",
      "lower\n",
      "levels\n",
      "find\n",
      "base\n",
      "riders\n",
      "preparing\n",
      "held\n",
      "fingers\n",
      "wrapped\n",
      "tied\n",
      "ring\n",
      "purse\n",
      "crowd\n",
      "boards\n",
      "ramp\n",
      "feeding\n",
      "fan\n",
      "stories\n",
      "smoke\n",
      "ship\n",
      "sheets\n",
      "balls\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "num_ans = 1000 # As supplied at the command line\n",
    "for i in range(num_ans):\n",
    "    vocab.append(cw[i][1])\n",
    "    print(vocab[i])\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(imgs, params, wtoi):\n",
    "\n",
    "    max_length = params['max_length']\n",
    "    N = len(imgs)\n",
    "\n",
    "    label_arrays = np.zeros((N, max_length), dtype='uint32')\n",
    "    label_length = np.zeros(N, dtype='uint32')\n",
    "    question_id = np.zeros(N, dtype='uint32')\n",
    "    question_counter = 0\n",
    "    for i,img in enumerate(imgs):\n",
    "        question_id[question_counter] = img['ques_id']\n",
    "        label_length[question_counter] = min(max_length, len(img['final_question'])) # record the length of this sequence\n",
    "        question_counter += 1\n",
    "        for k,w in enumerate(img['final_question']):\n",
    "            if k < max_length:\n",
    "                label_arrays[i,k] = wtoi[w]\n",
    "    \n",
    "    return label_arrays, label_length, question_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `encode_question()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443757\n"
     ]
    }
   ],
   "source": [
    "max_length = 26\n",
    "N = len(imgs_train)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arrays = np.zeros((N, max_length), dtype='uint32')\n",
    "label_length = np.zeros(N, dtype='uint32')\n",
    "question_id = np.zeros(N, dtype='uint32')\n",
    "question_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(label_arrays)\n",
    "print(label_length)\n",
    "print(question_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img in enumerate(imgs_train):\n",
    "    question_id[question_counter] = img['ques_id']\n",
    "    label_length[question_counter] = min(max_length, len(img['final_question'])) # record the length of this sequence\n",
    "    question_counter += 1\n",
    "    for k,w in enumerate(img['final_question']):\n",
    "        if k < max_length:\n",
    "            label_arrays[i,k] = wtoi[w]\n",
    "            #print(label_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answer(imgs, atoi):\n",
    "    N = len(imgs)\n",
    "    ans_arrays = np.zeros(N, dtype='uint32')\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        ans_arrays[i] = atoi[img['ans']]\n",
    "\n",
    "    return ans_arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `encode_answer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(imgs_train)\n",
    "ans_array = np.zeros(N, dtype='uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoi = {w:i+1 for i,w in enumerate(top_ans)}\n",
    "itoa = {i+1:w for i,w in enumerate(top_ans)}\n",
    "json.dump(itoa, open('ix_to_ans.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ans_arrays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1862b556a4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mans_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ans'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print(ans_array[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ans_arrays' is not defined"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    ans_arrays[i] = atoi[img['ans']]\n",
    "    #print(ans_array[i])\n",
    "    \n",
    "print(ans_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_mc_answer(imgs, atoi):\n",
    "    N = len(imgs)\n",
    "    mc_ans_arrays = np.zeros((N, 18), dtype='uint32')\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        for j, ans in enumerate(img['MC_ans']):\n",
    "            mc_ans_arrays[i,j] = atoi.get(ans, 0)\n",
    "    return mc_ans_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `encode__mc_answer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(imgs_train)\n",
    "mc_ans_arrays = np.zeros((N, 18), dtype='uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    for j, ans in enumerate(img['MC_ans']):\n",
    "        mc_ans_arrays[i,j] = atoi.get(ans, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_question(imgs, atoi):\n",
    "    new_imgs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        if atoi.get(img['ans'],len(atoi)+1) != len(atoi)+1:\n",
    "            new_imgs.append(img)\n",
    "\n",
    "    print ('question number reduce from %d to %d '%(len(imgs), len(new_imgs)))\n",
    "    return new_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `filter_question()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgs = []\n",
    "\n",
    "for i, img in enumerate(imgs_train):\n",
    "    if atoi.get(img['ans'],len(atoi)+1) != len(atoi)+1:\n",
    "        new_imgs.append(img)\n",
    "        #print(new_imgs[i])\n",
    "print(new_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[\n",
    "{'ques_id': 458752000, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What is this photo taken looking through?', 'ans': 'net', 'processed_tokens': ['what', 'is', 'this', 'photo', 'taken', 'looking', 'through', '?']},\n",
    "\n",
    "{'ques_id': 458752001, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What position is this man playing?', 'ans': 'pitcher', 'processed_tokens': ['what', 'position', 'is', 'this', 'man', 'playing', '?']}, \n",
    "\n",
    "{'ques_id': 458752002, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'What color is the players shirt?', 'ans': 'orange', 'processed_tokens': ['what', 'color', 'is', 'the', 'players', 'shirt', '?']}, \n",
    "\n",
    "{'ques_id': 458752003, 'img_path': 'train2014/COCO_train2014_000000458752.jpg', 'question': 'Is this man a professional baseball player?', 'ans': 'yes', 'processed_tokens': ['is', 'this', 'man', 'a', 'professional', 'baseball', 'player', '?']},\n",
    "\n",
    "{'ques_id': 262146000, 'img_path': 'train2014/COCO_train2014_000000262146.jpg', 'question': 'What color is the snow?', 'ans': 'white', 'processed_tokens': ['what', 'color', 'is', 'the', 'snow', '?']},\n",
    "\n",
    "{'ques_id': 262146001, 'img_path': 'train2014/COCO_train2014_000000262146.jpg', 'question': 'What is the person doing?', 'ans': 'skiing', 'processed_tokens': ['what', 'is', 'the', 'person', 'doing', '?']},\n",
    "\n",
    "{'ques_id': 262146002, 'img_path': 'train2014/COCO_train2014_000000262146.jpg', 'question': 'What color is the persons headwear?', 'ans': 'red', 'processed_tokens': ['what', 'color', 'is', 'the', 'persons', 'headwear', '?']}, \n",
    "\n",
    "{'ques_id': 524291000, 'img_path': 'train2014/COCO_train2014_000000524291.jpg', 'question': \"What is in the person's hand?\", 'ans': 'frisbee', 'processed_tokens': ['what', 'is', 'in', 'the', 'person', \"'s\", 'hand', '?']},\n",
    "\n",
    "{'ques_id': 524291001, 'img_path': 'train2014/COCO_train2014_000000524291.jpg', 'question': 'Is the dog waiting?', 'ans': 'yes', 'processed_tokens': ['is', 'the', 'dog', 'waiting', '?']},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question number reduce from 443757 to 401152 \n"
     ]
    }
   ],
   "source": [
    "print ('question number reduce from %d to %d '%(len(imgs_train), len(new_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unqiue_img(imgs):\n",
    "    count_img = {}\n",
    "    N = len(imgs)\n",
    "    img_pos = np.zeros(N, dtype='uint32')\n",
    "    for img in imgs:\n",
    "        count_img[img['img_path']] = count_img.get(img['img_path'], 0) + 1\n",
    "\n",
    "    unique_img = [w for w,n in count_img.items()]\n",
    "    imgtoi = {w:i+1 for i,w in enumerate(unique_img)} # add one for torch, since torch start from 1.\n",
    "\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img_pos[i] = imgtoi.get(img['img_path'])\n",
    "\n",
    "    return unique_img, img_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrsponding short snippets for `get_unique_img()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_img = {}\n",
    "N = len(imgs_train)\n",
    "img_pos = np.zeros(N, dtype='uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in imgs_train:\n",
    "    count_img[img['img_path']] = count_img.get(img['img_path'], 0) + 1\n",
    "    \n",
    "print(count_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'train2014/COCO_train2014_000000458752.jpg': 4,\n",
    "'train2014/COCO_train2014_000000262146.jpg': 3,\n",
    "'train2014/COCO_train2014_000000524291.jpg': 3,\n",
    "'train2014/COCO_train2014_000000393221.jpg': 3,\n",
    "'train2014/COCO_train2014_000000393223.jpg': 4,\n",
    "'train2014/COCO_train2014_000000393224.jpg': 6,\n",
    "'train2014/COCO_train2014_000000524297.jpg': 4,\n",
    "'train2014/COCO_train2014_000000393227.jpg': 5,\n",
    "'train2014/COCO_train2014_000000131084.jpg': 3,\n",
    "'train2014/COCO_train2014_000000131074.jpg': 6,\n",
    "'train2014/COCO_train2014_000000393230.jpg': 11,\n",
    "'train2014/COCO_train2014_000000262187.jpg': 4, \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_img = [w for w,n in count_img.items()]\n",
    "imgtoi = {w:i+1 for i,w in enumerate(unique_img)} # add one for torch, since torch start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(unique_img)\n",
    "#print(imgtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "['train2014/COCO_train2014_000000458752.jpg', 'train2014/COCO_train2014_000000262146.jpg', 'train2014/COCO_train2014_000000524291.jpg', 'train2014/COCO_train2014_000000393221.jpg', 'train2014/COCO_train2014_000000393223.jpg', 'train2014/COCO_train2014_000000393224.jpg', 'train2014/COCO_train2014_000000524297.jpg', 'train2014/COCO_train2014_000000393227.jpg', 'train2014/COCO_train2014_000000131084.jpg', 'train2014/COCO_train2014_000000131074.jpg', 'train2014/COCO_train2014_000000393230.jpg', 'train2014/COCO_train2014_000000131087.jpg', 'train2014/COCO_train2014_000000131075.jpg', 'train2014/COCO_train2014_000000137045.jpg', 'train2014/COCO_train2014_000000131093.jpg', 'train2014/COCO_train2014_000000524311.jpg', 'train2014/COCO_train2014_000000000025.jpg', 'train2014/COCO_train2014_000000524314.jpg', 'train2014/COCO_train2014_000000262171.jpg', 'train2014/COCO_train2014_000000262172.jpg', 'train2014/COCO_train2014_000000131101.jpg', 'train2014/COCO_train2014_000000000030.jpg', 'train2014/COCO_train2014_000000524320.jpg', 'train2014/COCO_train2014_000000240304.jpg', 'train2014/COCO_train2014_000000000034.jpg', 'train2014/COCO_train2014_000000393251.jpg', 'train2014/COCO_train2014_000000262180.jpg', 'train2014/COCO_train2014_000000524325.jpg', 'train2014/COCO_train2014_000000043697.jpg', 'train2014/COCO_train2014_000000262184.jpg', 'train2014/COCO_train2014_000000131113.jpg', 'train2014/COCO_train2014_000000262187.jpg', 'train2014/COCO_train2014_000000131118.jpg', 'train2014/COCO_train2014_000000262191.jpg', 'train2014/COCO_train2014_000000000049.jpg', 'train2014/COCO_train2014_000000524338.jpg', 'train2014/COCO_train2014_000000213863.jpg', \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(imgtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'train2014/COCO_train2014_000000458752.jpg': 1, 'train2014/COCO_train2014_000000262146.jpg': 2, 'train2014/COCO_train2014_000000524291.jpg': 3, 'train2014/COCO_train2014_000000393221.jpg': 4, 'train2014/COCO_train2014_000000393223.jpg': 5, 'train2014/COCO_train2014_000000393224.jpg': 6, 'train2014/COCO_train2014_000000524297.jpg': 7, 'train2014/COCO_train2014_000000393227.jpg': 8, 'train2014/COCO_train2014_000000131084.jpg': 9, 'train2014/COCO_train2014_000000131074.jpg': 10, 'train2014/COCO_train2014_000000393230.jpg': 11, 'train2014/COCO_train2014_000000131087.jpg': 12, 'train2014/COCO_train2014_000000131075.jpg': 13, 'train2014/COCO_train2014_000000137045.jpg': 14, 'train2014/COCO_train2014_000000131093.jpg': 15, 'train2014/COCO_train2014_000000524311.jpg': 16, 'train2014/COCO_train2014_000000000025.jpg': 17, 'train2014/COCO_train2014_000000524314.jpg': 18, 'train2014/COCO_train2014_000000262171.jpg': 19, 'train2014/COCO_train2014_000000262172.jpg': 20, 'train2014/COCO_train2014_000000131101.jpg': 21, 'train2014/COCO_train2014_000000000030.jpg': 22, 'train2014/COCO_train2014_000000524320.jpg': 23, 'train2014/COCO_train2014_000000240304.jpg': 24, 'train2014/COCO_train2014_000000000034.jpg': 25, 'train2014/COCO_train2014_000000393251.jpg': 26, 'train2014/COCO_train2014_000000262180.jpg': 27, 'train2014/COCO_train2014_000000524325.jpg': 28, 'train2014/COCO_train2014_000000043697.jpg': 29, 'train2014/COCO_train2014_000000262184.jpg': 30, 'train2014/COCO_train2014_000000131113.jpg': 31, 'train2014/COCO_train2014_000000262187.jpg': 32, 'train2014/COCO_train2014_000000131118.jpg': 33, 'train2014/COCO_train2014_000000262191.jpg': 34\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     1     1 ... 82783 82783 82783]\n"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(imgs_train):\n",
    "    img_pos[i] = imgtoi.get(img['img_path'])\n",
    "    #print(img_pos[i])\n",
    "    \n",
    "print(img_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
